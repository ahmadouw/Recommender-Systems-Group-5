{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "path_to_data = '../shared/data/project/validation/'\n",
    "path_to_NN = '../project/results_NN.csv'\n",
    "path_to_UU = '../project/results_UU.csv'\n",
    "path_to_CO = './results_content.csv'\n",
    "dataset_type = 'one_hour' # all_sorted, one_day, one_hour, one_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-05dd38bf875f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdatetime\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mmodel_nn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Recommender-Systems-Group-5/src/model_nn.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;31m# import scikitplot as skplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mseaborn\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mimblearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover_sampling\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSMOTE\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mimblearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipeline\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mPipeline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import datetime\n",
    "import model_nn\n",
    "import pandas as pd\n",
    "\n",
    "from dataprep import import_data\n",
    "import dataprep\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = [\n",
    "            # \"text_tokens\",\n",
    "            \"hashtags\",\n",
    "            # \"tweet_id\",\n",
    "            \"present_media\",\n",
    "            \"present_links\",\n",
    "            \"present_domains\",\n",
    "            \"tweet_type\",\n",
    "            # \"language\",\n",
    "            # \"tweet_timestamp\",\n",
    "            # \"engaged_with_user_id\",\n",
    "            # \"engaged_with_user_follower_count\",\n",
    "            \"engaged_with_user_following_count\",\n",
    "            \"engaged_with_user_is_verified\",\n",
    "            \"engaged_with_user_account_creation\",\n",
    "            # \"engaging_user_id\",\n",
    "            # \"enaging_user_follower_count\",\n",
    "            # \"enaging_user_following_count\",\n",
    "            # \"enaging_user_is_verified\",\n",
    "            # \"enaging_user_account_creation\",\n",
    "            \"engagee_follows_engager\"\n",
    "    ]\n",
    "\n",
    "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\\\n",
    "                \"tweet_type\",\"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\\\n",
    "               \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
    "               \"engaging_user_id\", \"enaging_user_follower_count\", \"enaging_user_following_count\", \"enaging_user_is_verified\",\\\n",
    "               \"enaging_user_account_creation\", \"engagee_follows_engager\"]\n",
    "\n",
    "all_features_to_idx = dict(zip(all_features, range(len(all_features))))\n",
    "\n",
    "def parse_input_line(line):\n",
    "        \n",
    "    features = line #.split(\"\\x01\")\n",
    "    \n",
    "    hashtags = features[all_features_to_idx['hashtags']]\n",
    "    present_media = features[all_features_to_idx['present_media']]\n",
    "    present_links = features[all_features_to_idx['present_links']]\n",
    "    present_domains = features[all_features_to_idx['present_domains']]\n",
    "    tweet_type = features[all_features_to_idx['tweet_type']]\n",
    "    engaged_with_user_following_count = features[all_features_to_idx['engaged_with_user_following_count']]\n",
    "    engaged_with_user_is_verified = features[all_features_to_idx['engaged_with_user_is_verified']]\n",
    "    engaged_with_user_account_creation = features[all_features_to_idx['engaged_with_user_account_creation']]\n",
    "    engagee_follows_engager = features[all_features_to_idx['engagee_follows_engager']]\n",
    "    \n",
    "    return (hashtags, present_media, present_links, present_domains, tweet_type, engaged_with_user_following_count, engaged_with_user_is_verified, engaged_with_user_account_creation, engagee_follows_engager)\n",
    "\n",
    "\n",
    "def evaluate_test_set():\n",
    "    expanded_path = os.path.expanduser(path_to_data)\n",
    "    part_files = [os.path.join(expanded_path, f) for f in os.listdir(expanded_path) if dataset_type in f]\n",
    "    part_files = sorted(part_files, key = lambda x:x[-5:]) \n",
    "        \n",
    "    with open('results_NN.csv', 'w') as output:\n",
    "        for file in part_files:\n",
    "            with open(file, 'r') as f:\n",
    "                linereader = csv.reader(f, delimiter='\\x01')\n",
    "                last_timestamp = None\n",
    "                df = pd.DataFrame(columns=all_features)\n",
    "                i = 0\n",
    "                for row in linereader:\n",
    "                    df.loc[i] = row[:20]\n",
    "                    i += 1\n",
    "                df_complete = df.copy()\n",
    "                df = df.loc[:, used_features]\n",
    "                df = dataprep.transform_data(df)\n",
    "                scale = StandardScaler()\n",
    "                #df = scale.fit_transform(df)\n",
    "                #df = pd.DataFrame(columns=all_features, data=df)\n",
    "                df = pd.DataFrame(scale.fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "                for index, row in df.iterrows():\n",
    "                    tweet_id = df_complete.iloc[[index]][\"tweet_id\"]\n",
    "                    user_id = df_complete.iloc[[index]][\"engaging_user_id\"]\n",
    "                    #feature_tuple = parse_input_line(row)   \n",
    "                    #reply_pred = reply_pred_model(features) # reply_model\n",
    "                    #retweet_pred = retweet_pred_model(features) # retweet_model\n",
    "                    #quote_pred = quote_pred_model(features) # pred_model\n",
    "                    #fav_pred = fav_pred_model(features) # fav_model\n",
    "                    \n",
    "                    #print(feature_tuple)\n",
    "                    #print(df.iloc[[index]][\"engaged_with_user_following_count\"])\n",
    "                    reply_pred = model_nn.reply_pred_model(df.iloc[[index]])\n",
    "                    retweet_pred = model_nn.retweet_pred_model(df.iloc[[index]])\n",
    "                    quote_pred = model_nn.quote_pred_model(df.iloc[[index]])\n",
    "                    fav_pred = model_nn.fav_pred_model(df.iloc[[index]])\n",
    "                    \n",
    "                    \n",
    "                    # print(str(tweet_timestamp))\n",
    "                    #print(str(reply_pred)+\" \"+str(retweet_pred)+\" \"+str(quote_pred)+\" \"+str(fav_pred))\n",
    "                    output.write(f'{tweet_id.values[0]},{user_id.values[0]},{reply_pred[0]},{retweet_pred[0]},{quote_pred[0]},{fav_pred[0]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_content\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import dataprep\n",
    "\n",
    "path_to_data = '../shared/data/project/validation/'\n",
    "dataset_type = 'one_hour' # all_sorted, one_day, one_hour, one_week\n",
    "\n",
    "\n",
    "used_features = model_content.used_features\n",
    "\n",
    "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\", \\\n",
    "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\", \\\n",
    "                \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\",\n",
    "                \"engaged_with_user_account_creation\", \\\n",
    "                \"engaging_user_id\", \"enaging_user_follower_count\", \"enaging_user_following_count\",\n",
    "                \"enaging_user_is_verified\", \\\n",
    "                \"enaging_user_account_creation\", \"engagee_follows_engager\"]\n",
    "\n",
    "all_features_to_idx = dict(zip(all_features, range(len(all_features))))\n",
    "\n",
    "\n",
    "def parse_input_line(line):\n",
    "    features = line  # .split(\"\\x01\")\n",
    "\n",
    "    hashtags = features[all_features_to_idx['hashtags']]\n",
    "    present_media = features[all_features_to_idx['present_media']]\n",
    "    present_links = features[all_features_to_idx['present_links']]\n",
    "    present_domains = features[all_features_to_idx['present_domains']]\n",
    "    tweet_type = features[all_features_to_idx['tweet_type']]\n",
    "    engaged_with_user_following_count = features[all_features_to_idx['engaged_with_user_following_count']]\n",
    "    engaged_with_user_is_verified = features[all_features_to_idx['engaged_with_user_is_verified']]\n",
    "    engaged_with_user_account_creation = features[all_features_to_idx['engaged_with_user_account_creation']]\n",
    "    engagee_follows_engager = features[all_features_to_idx['engagee_follows_engager']]\n",
    "\n",
    "    return (hashtags, present_media, present_links, present_domains, tweet_type, engaged_with_user_following_count,\n",
    "            engaged_with_user_is_verified, engaged_with_user_account_creation, engagee_follows_engager)\n",
    "\n",
    "\n",
    "def evaluate_test_set():\n",
    "    expanded_path = os.path.expanduser(path_to_data)\n",
    "    part_files = [os.path.join(expanded_path, f) for f in os.listdir(expanded_path) if dataset_type in f]\n",
    "    part_files = sorted(part_files, key=lambda x: x[-5:])\n",
    "    with open('results_CO.csv', 'w') as output:\n",
    "        for file in part_files:\n",
    "            with open(file, 'r') as f:\n",
    "                print(\"doing\", file)\n",
    "                linereader = csv.reader(f, delimiter='\\x01')\n",
    "                last_timestamp = None\n",
    "                df = pd.DataFrame(columns=all_features)\n",
    "                i = 0\n",
    "                for row in linereader:\n",
    "                    df.loc[i] = row[:20]\n",
    "                    i += 1\n",
    "                    print(\"doing line \", i)\n",
    "                    if i > 100000:\n",
    "                        break\n",
    "\n",
    "                df_complete = df.copy()\n",
    "                df = df.loc[:, used_features]\n",
    "                df = dataprep.transform_data(df)\n",
    "\n",
    "                df = pd.DataFrame(df.values, columns=df.columns, index=df.index)\n",
    "\n",
    "                print(\"got pd\")\n",
    "                for index, row in df.iterrows():\n",
    "                    tweet_id = df_complete.iloc[[index]][\"tweet_id\"]\n",
    "                    user_id = df_complete.iloc[[index]][\"engaging_user_id\"]\n",
    "\n",
    "                    reply_pred = model_content.reply_pred_model(df.iloc[[index]])\n",
    "                    retweet_pred = model_content.retweet_pred_model(df.iloc[[index]])\n",
    "                    quote_pred = model_content.quote_pred_model(df.iloc[[index]])\n",
    "                    fav_pred = model_content.fav_pred_model(df.iloc[[index]])\n",
    "\n",
    "                    output.write(\n",
    "                        f'{tweet_id.values[0]},{user_id.values[0]},{reply_pred[0]},{retweet_pred[0]},{quote_pred[0]},{fav_pred[0]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-To-User Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Model class\n",
    "%run -i \"u2u.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from stored files\n",
    "u2u_reply = load(\"model_content/u2u_reply.joblib\")\n",
    "u2u_retweet = load(\"model_content/u2u_retweet.joblib\")\n",
    "u2u_quote = load(\"model_content/u2u_retweet_with_comment.joblib\")\n",
    "u2u_fav = load(\"model_content/u2u_like.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator class that computes the f1 score. Use in evaluate_test_set!\n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.tp = 0\n",
    "        self.tn = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        \n",
    "    def add(self, interaction, prediction):\n",
    "        true = 0 if interaction == \"\" else 1\n",
    "        if true == prediction and true == 0:\n",
    "            self.tn += 1\n",
    "        elif true == prediction and true == 1:\n",
    "            self.tp += 1\n",
    "        elif true != prediction and true == 0:\n",
    "            self.fp += 1\n",
    "        elif true != prediction and true == 1:\n",
    "            self.fn += 1\n",
    "            \n",
    "    def get_f1(self):\n",
    "        try:\n",
    "            return self.tp / (self.tp + 0.5 * (self.fn + self.fp))\n",
    "        except:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-To-User Model\n",
    "\n",
    "path_to_data = '../shared/data/project/validation/'\n",
    "dataset_type = 'one_hour' # all_sorted, one_day, one_hour, one_week\n",
    "\n",
    "\n",
    "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\\\n",
    "                \"tweet_type\",\"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\\\n",
    "               \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
    "               \"engaging_user_id\", \"enaging_user_follower_count\", \"enaging_user_following_count\", \"enaging_user_is_verified\",\\\n",
    "               \"enaging_user_account_creation\", \"engagee_follows_engager\", \"retweet\", \"reply\", \"like\", \"retweet_with_comment\"]\n",
    "\n",
    "all_features_to_idx = dict(zip(all_features, range(len(all_features))))\n",
    "\n",
    "\n",
    "# adapted to also return the true interaction values\n",
    "def parse_input_line(line):\n",
    "    features = line #.split(\"\\x01\")\n",
    "    tweet_id = features[all_features_to_idx['tweet_id']]\n",
    "    user_id = features[all_features_to_idx['engaging_user_id']]\n",
    "    input_feats = features[all_features_to_idx['text_tokens']]\n",
    "    tweet_timestamp = features[all_features_to_idx['tweet_timestamp']]\n",
    "    retweet = features[all_features_to_idx['retweet']]\n",
    "    reply = features[all_features_to_idx['reply']]\n",
    "    like = features[all_features_to_idx['like']]\n",
    "    retweet_with_comment = features[all_features_to_idx['retweet_with_comment']]\n",
    "\n",
    "    return tweet_id, user_id, input_feats, tweet_timestamp, retweet, reply, like, retweet_with_comment\n",
    "\n",
    "\n",
    "#adapted to return the f1 values for each interaction category\n",
    "def evaluate_test_set():\n",
    "    expanded_path = os.path.expanduser(path_to_data)\n",
    "    part_files = [os.path.join(expanded_path, f) for f in os.listdir(expanded_path) if dataset_type in f]\n",
    "    part_files = sorted(part_files, key = lambda x:x[-5:]) \n",
    "    \n",
    "    reply_eval = Evaluator()\n",
    "    retweet_eval = Evaluator()\n",
    "    quote_eval = Evaluator()\n",
    "    fav_eval = Evaluator()\n",
    "    \n",
    "        \n",
    "    with open('results_UU.csv', 'w') as output:\n",
    "        for file in part_files:\n",
    "            with open(file, 'r') as f:\n",
    "                linereader = csv.reader(f, delimiter='\\x01')\n",
    "                last_timestamp = None\n",
    "                for row in linereader:\n",
    "                    tweet_id, user_id, features, tweet_timestamp, retweet, reply, fav, quote = parse_input_line(row)   \n",
    "                    reply_pred = u2u_reply.predict(user_id, tweet_id, binary=True) # reply_model\n",
    "                    retweet_pred = u2u_retweet.predict(user_id, tweet_id, binary=True) # retweet_model\n",
    "                    quote_pred = u2u_quote.predict(user_id, tweet_id, binary=True) # pred_model\n",
    "                    fav_pred = u2u_fav.predict(user_id, tweet_id, binary=True) # fav_model\n",
    "                    \n",
    "                    reply_eval.add(reply, reply_pred)\n",
    "                    retweet_eval.add(retweet, retweet_pred)\n",
    "                    quote_eval.add(quote, quote_pred)\n",
    "                    fav_eval.add(fav, fav_pred)\n",
    "                    \n",
    "                    # print(str(tweet_timestamp))\n",
    "                    # print(str(reply_pred)+\" \"+str(retweet_pred)+\" \"+str(quote_pred)+\" \"+str(fav_pred))\n",
    "                    \n",
    "                    output.write(f'{tweet_id},{user_id},{reply_pred},{retweet_pred},{quote_pred},{fav_pred}\\n')\n",
    "        return reply_eval.get_f1(), retweet_eval.get_f1(), quote_eval.get_f1(), fav_eval.get_f1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_eval, retweet_eval, quote_eval, fav_eval = evaluate_test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get ground truth\n",
    "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\\\n",
    "                \"tweet_type\",\"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\\\n",
    "               \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
    "               \"engaging_user_id\", \"enaging_user_follower_count\", \"enaging_user_following_count\", \"enaging_user_is_verified\",\\\n",
    "               \"enaging_user_account_creation\", \"engagee_follows_engager\", \"retweet\", \"reply\", \"retweet_with_comment\", \"like\"]\n",
    "\n",
    "all_features_to_idx = dict(zip(all_features, range(len(all_features))))\n",
    "\n",
    "def parse_input_line(line):\n",
    "    features = line #.split(\"\\x01\")\n",
    "    retweet = features[all_features_to_idx['retweet']]\n",
    "    reply = features[all_features_to_idx['reply']]\n",
    "    retweet_with_comment = features[all_features_to_idx['retweet_with_comment']]\n",
    "    like = features[all_features_to_idx['like']]\n",
    "\n",
    "    return retweet, reply, retweet_with_comment, like\n",
    "\n",
    "def read_ground_truth():\n",
    "    expanded_path = os.path.expanduser(path_to_data)\n",
    "    part_files = [os.path.join(expanded_path, f) for f in os.listdir(expanded_path) if dataset_type in f]\n",
    "    part_files = sorted(part_files, key = lambda x:x[-5:]) \n",
    "    \n",
    "    retweets = []\n",
    "    replies = []\n",
    "    retweets_with_comment = []\n",
    "    likes = []\n",
    "        \n",
    "    for file in part_files:\n",
    "            with open(file, 'r') as f:\n",
    "                linereader = csv.reader(f, delimiter='\\x01')\n",
    "                last_timestamp = None\n",
    "                for row in linereader:\n",
    "                    retweet, reply, retweet_with_comment, like = parse_input_line(row)\n",
    "                    retweets.append(0 if retweet == '' else 1)\n",
    "                    replies.append(0 if reply == '' else 1)\n",
    "                    retweets_with_comment.append(0 if retweet_with_comment == '' else 1)\n",
    "                    likes.append(0 if like == '' else 1)\n",
    "    return retweets, replies, likes, retweets_with_comment\n",
    "                    \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_gt, reply_gt, retweet_with_comment_gt, like_gt = read_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions from csv\n",
    "import csv\n",
    "def read_class_predictions(filename):\n",
    "    \n",
    "    retweets = []\n",
    "    replies = []\n",
    "    retweets_with_comment = []\n",
    "    likes = []\n",
    "        \n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            retweet = float(row[2])\n",
    "            reply = float(row[3])\n",
    "            retweet_with_comment = float(row[4])\n",
    "            like = float(row[5])\n",
    "            retweets.append(0 if retweet < 0.5 else 1)\n",
    "            replies.append(0 if reply < 0.5 else 1)\n",
    "            retweets_with_comment.append(0 if retweet_with_comment < 0.5 else 1)\n",
    "            likes.append(0 if like < 0.5 else 1)\n",
    "    return retweets, replies, likes, retweets_with_comment\n",
    "def read_predictions(filename):\n",
    "    \n",
    "    retweets = []\n",
    "    replies = []\n",
    "    retweets_with_comment = []\n",
    "    likes = []\n",
    "        \n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            retweet = float(row[2])\n",
    "            reply = float(row[3])\n",
    "            retweet_with_comment = float(row[4])\n",
    "            like = float(row[5])\n",
    "            retweets.append(retweet)\n",
    "            replies.append(reply)\n",
    "            retweets_with_comment.append(retweet_with_comment)\n",
    "            likes.append(like)\n",
    "    return retweets, replies, likes, retweets_with_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get interaction class labels\n",
    "#retweet_NN, reply_NN, retweet_with_comment_NN, like_NN = read_class_predictions(path_to_NN)\n",
    "retweet_CO, reply_CO, retweet_with_comment_CO, like_CO = read_class_predictions(path_to_CO)\n",
    "#retweet_UU, reply_UU, retweet_with_comment_UU, like_UU = read_class_predictions(path_to_UU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get interaction probabilities\n",
    "# retweet_NN_p, reply_NN_p, retweet_with_comment_NN_p, like_NN_p = read_predictions(path_to_NN)\n",
    "retweet_CO_p, reply_CO_p, retweet_with_comment_CO_p, like_CO_p = read_predictions(path_to_CO)\n",
    "# retweet_UU_p, reply_UU_p, retweet_with_comment_UU_p, like_UU_p = read_predictions(path_to_UU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of correctly labelled interactions\n",
    "def correct_interaction(pred, gt):\n",
    "    pos = 0\n",
    "    correct = 0\n",
    "    for p, g in zip(pred,gt):\n",
    "        if g == 1:\n",
    "            pos += 1\n",
    "            if g == p:\n",
    "                correct += 1\n",
    "    print(correct)\n",
    "    print(pos)\n",
    "    return correct/pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retweet_NN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-80bc6be5be14>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcorrect_interaction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mretweet_NN\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretweet_gt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'retweet_NN' is not defined"
     ]
    }
   ],
   "source": [
    "correct_interaction(retweet_NN, retweet_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reply_NN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-26-0fc544c7bc89>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcorrect_interaction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreply_NN\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreply_gt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'reply_NN' is not defined"
     ]
    }
   ],
   "source": [
    "correct_interaction(reply_NN, reply_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_interaction(retweet_with_comment_NN, retweet_with_comment_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_interaction(like_NN, like_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, log_loss\n",
    "\n",
    "def calculate_ctr(gt):\n",
    "    positive = len([x for x in gt if x == 1])\n",
    "    ctr = positive/float(len(gt))\n",
    "    return ctr\n",
    "\n",
    "def compute_rce(pred, gt):\n",
    "    cross_entropy = log_loss(gt, pred)\n",
    "    data_ctr = calculate_ctr(gt)\n",
    "    strawman_cross_entropy = log_loss(gt, [data_ctr for _ in range(len(gt))])\n",
    "    return (1.0 - cross_entropy/strawman_cross_entropy)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retweet_NN_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-103f652581a0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# NN Evaluation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mrce_NN_retweet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompute_rce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mretweet_NN_p\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretweet_gt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0maverage_precision_NN_retweet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maverage_precision_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mretweet_gt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretweet_NN_p\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mrce_NN_reply\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompute_rce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreply_NN_p\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreply_gt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0maverage_precision_NN_reply\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maverage_precision_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreply_gt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreply_NN_p\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'retweet_NN_p' is not defined"
     ]
    }
   ],
   "source": [
    "# NN Evaluation\n",
    "rce_NN_retweet = compute_rce(retweet_NN_p, retweet_gt)\n",
    "average_precision_NN_retweet = average_precision_score(retweet_gt, retweet_NN_p)\n",
    "rce_NN_reply = compute_rce(reply_NN_p, reply_gt)\n",
    "average_precision_NN_reply = average_precision_score(reply_gt, reply_NN_p)\n",
    "rce_NN_retweet_with_comment = compute_rce(retweet_with_comment_NN_p, retweet_with_comment_gt)\n",
    "average_precision_NN_retweet_with_comment = average_precision_score(retweet_with_comment_gt, retweet_with_comment_NN_p)\n",
    "rce_NN_like = compute_rce(like_NN_p, like_gt)\n",
    "average_precision_NN_like = average_precision_score(like_gt, like_NN_p)\n",
    "print(\"RCE Retweet:\" + str(rce_NN_retweet) + \"RCE Reply:\" + str(rce_NN_reply) + \"RCE Retweet with Comment\" + str(rce_NN_retweet_with_comment) +  \"RCE Like:\" + str(rce_NN_like))\n",
    "print(\"AP Retweet:\" + str(average_precision_NN_retweet) + \"AP Reply:\" + str(average_precision_NN_reply) + \"AP Retweet with Comment\" + str(average_precision_NN_retweet_with_comment) +  \"AP Like:\" + str(average_precision_NN_like))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print(precision_recall_fscore_support(retweet_gt, retweet_NN, average='micro'))\n",
    "print(precision_recall_fscore_support(reply_gt, reply_NN, average='micro'))\n",
    "print(precision_recall_fscore_support(retweet_with_comment_gt, retweet_with_comment_NN, average='micro'))\n",
    "print(precision_recall_fscore_support(like_gt, like_NN, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1,4)\n",
    "\n",
    "prec, recall, _ = precision_recall_curve(retweet_gt, retweet_NN)\n",
    "retweet_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[0])\n",
    "prec, recall, _ = precision_recall_curve(reply_gt, reply_NN)\n",
    "reply_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[1])\n",
    "prec, recall, _ = precision_recall_curve(retweet_with_comment_gt, retweet_with_comment_NN)\n",
    "retweet_with_comment_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[2])\n",
    "prec, recall, _ = precision_recall_curve(like_gt, like_NN)\n",
    "like_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCE Retweet:-10.49902744016007RCE Reply:-36.8845006334203RCE Retweet with Comment-214.82347457879482RCE Like:-640.9006161047103\n",
      "AP Retweet:0.016827254347158264AP Reply:0.06097788503905255AP Retweet with Comment0.49957294806985864AP Like:0.006651413481586675\n"
     ]
    }
   ],
   "source": [
    "# CO Evaluation\n",
    "rce_CO_retweet = compute_rce(retweet_CO_p, retweet_gt)\n",
    "average_precision_CO_retweet = average_precision_score(retweet_gt, retweet_CO_p)\n",
    "rce_CO_reply = compute_rce(reply_CO_p, reply_gt)\n",
    "average_precision_CO_reply = average_precision_score(reply_gt, reply_CO_p)\n",
    "rce_CO_retweet_with_comment = compute_rce(retweet_with_comment_CO_p, retweet_with_comment_gt)\n",
    "average_precision_CO_retweet_with_comment = average_precision_score(retweet_with_comment_gt, retweet_with_comment_CO_p)\n",
    "rce_CO_like = compute_rce(like_CO_p, like_gt)\n",
    "average_precision_CO_like = average_precision_score(like_gt, like_CO_p)\n",
    "print(\"RCE Retweet:\" + str(rce_CO_retweet) + \"RCE Reply:\" + str(rce_CO_reply) + \"RCE Retweet with Comment\" + str(rce_CO_retweet_with_comment) +  \"RCE Like:\" + str(rce_CO_like))\n",
    "print(\"AP Retweet:\" + str(average_precision_CO_retweet) + \"AP Reply:\" + str(average_precision_CO_reply) + \"AP Retweet with Comment\" + str(average_precision_CO_retweet_with_comment) +  \"AP Like:\" + str(average_precision_CO_like))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9775412730957488, 0.9775412730957488, 0.9775412730957488, None)\n",
      "(0.9291631605769345, 0.9291631605769345, 0.9291631605769345, None)\n",
      "(0.5920807803074297, 0.5920807803074297, 0.5920807803074297, None)\n",
      "(0.9940623629662112, 0.9940623629662112, 0.9940623629662112, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print(precision_recall_fscore_support(retweet_gt, retweet_CO, average='micro'))\n",
    "print(precision_recall_fscore_support(reply_gt, reply_CO, average='micro'))\n",
    "print(precision_recall_fscore_support(retweet_with_comment_gt, retweet_with_comment_CO, average='micro'))\n",
    "print(precision_recall_fscore_support(like_gt, like_CO, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "fig, axs = plt.subplots(1,4)\n",
    "\n",
    "prec, recall, _ = precision_recall_curve(retweet_gt, retweet_CO)\n",
    "retweet_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[0])\n",
    "prec, recall, _ = precision_recall_curve(reply_gt, reply_CO)\n",
    "reply_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[0])\n",
    "prec, recall, _ = precision_recall_curve(retweet_with_comment_gt, retweet_with_comment_CO)\n",
    "retweet_with_comment_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[0])\n",
    "prec, recall, _ = precision_recall_curve(like_gt, like_CO)\n",
    "like_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UU Evaluation\n",
    "rce_UU_retweet = compute_rce(retweet_UU_p, retweet_gt)\n",
    "average_precision_UU_retweet = average_precision_score(retweet_gt, retweet_UU_p)\n",
    "rce_UU_reply = compute_rce(reply_UU_p, reply_gt)\n",
    "average_precision_UU_reply = average_precision_score(reply_gt, reply_UU_p)\n",
    "rce_UU_retweet_with_comment = compute_rce(retweet_with_comment_UU_p, retweet_with_comment_gt)\n",
    "average_precision_UU_retweet_with_comment = average_precision_score(retweet_with_comment_gt, retweet_with_comment_UU_p)\n",
    "rce_UU_like = compute_rce(like_UU_p, like_gt)\n",
    "average_precision_UU_like = average_precision_score(like_gt, like_UU_p)\n",
    "print(\"RCE Retweet:\" + str(rce_UU_retweet) + \"RCE Reply:\" + str(rce_UU_reply) + \"RCE Retweet with Comment\" + str(rce_UU_retweet_with_comment) +  \"RCE Like:\" + str(rce_UU_like))\n",
    "print(\"AP Retweet:\" + str(average_precision_UU_retweet) + \"AP Reply:\" + str(average_precision_UU_reply) + \"AP Retweet with Comment\" + str(average_precision_UU_retweet_with_comment) +  \"AP Like:\" + str(average_precision_UU_like))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retweet_UU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-32-b5d9d07258ec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetrics\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mprecision_recall_fscore_support\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprecision_recall_fscore_support\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mretweet_gt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretweet_UU\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maverage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'micro'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprecision_recall_fscore_support\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreply_gt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreply_UU\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maverage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'micro'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprecision_recall_fscore_support\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mretweet_with_comment_gt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretweet_with_comment_UU\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maverage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'micro'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprecision_recall_fscore_support\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlike_gt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlike_UU\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maverage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'micro'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'retweet_UU' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print(precision_recall_fscore_support(retweet_gt, retweet_UU, average='micro'))\n",
    "print(precision_recall_fscore_support(reply_gt, reply_UU, average='micro'))\n",
    "print(precision_recall_fscore_support(retweet_with_comment_gt, retweet_with_comment_UU, average='micro'))\n",
    "print(precision_recall_fscore_support(like_gt, like_UU, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "fig, axs = plt.subplots(1,4)\n",
    "\n",
    "prec, recall, _ = precision_recall_curve(retweet_gt, retweet_UU)\n",
    "retweet_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[0])\n",
    "prec, recall, _ = precision_recall_curve(reply_gt, reply_UU)\n",
    "reply_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[1])\n",
    "prec, recall, _ = precision_recall_curve(retweet_with_comment_gt, retweet_with_comment_UU)\n",
    "retweet_with_comment_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[2])\n",
    "prec, recall, _ = precision_recall_curve(like_gt, like_UU)\n",
    "like_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=axs[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}